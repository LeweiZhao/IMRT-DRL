{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e468d1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating rinds.. This step may take some time due to dilation\n",
      "Optimization structures created!!\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "Creating BEV..\n",
      "Loading sparse influence matrix...\n",
      "Done\n",
      "Objective Start\n",
      "Objective done\n",
      "Constraints Start\n",
      "Constraints done\n",
      "Running Optimization..\n",
      "Optimal value: 68045.23554216835\n",
      "Elapsed time: 12.154155015945435 seconds\n"
     ]
    }
   ],
   "source": [
    "import env\n",
    "import rl_utils\n",
    "import random\n",
    "data_path = r'D:\\MyJupyter\\RL_BAO\\Data'\n",
    "env = env.PortPyEnv(data_path = data_path, step_max_num = 3)\n",
    "\n",
    "state = env.reset()\n",
    "print(state)\n",
    "next_state, reward, done, _ = env.step(71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84c1fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758e50c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.Reward(state, next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "222b7b72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BEV..\n",
      "Loading sparse influence matrix...\n",
      "Done\n",
      "Objective Start\n",
      "Objective done\n",
      "Constraints Start\n",
      "Constraints done\n",
      "Running Optimization..\n",
      "Optimal value: 293907.0952384332\n",
      "Elapsed time: 11.585367202758789 seconds\n",
      "Creating BEV..\n",
      "Loading sparse influence matrix...\n",
      "Done\n",
      "Objective Start\n",
      "Objective done\n",
      "Constraints Start\n",
      "Constraints done\n",
      "Running Optimization..\n",
      "Optimal value: 51052.14172480058\n",
      "Elapsed time: 19.068679332733154 seconds\n",
      "Creating BEV..\n",
      "Loading sparse influence matrix...\n",
      "Done\n",
      "Objective Start\n",
      "Objective done\n",
      "Constraints Start\n",
      "Constraints done\n",
      "Running Optimization..\n",
      "Optimal value: 29268.711918455727\n",
      "Elapsed time: 25.87359857559204 seconds\n",
      "Creating BEV..\n",
      "Loading sparse influence matrix...\n",
      "Done\n",
      "Objective Start\n",
      "Objective done\n",
      "Constraints Start\n",
      "Constraints done\n",
      "Running Optimization..\n",
      "Optimal value: 29268.712124010934\n",
      "Elapsed time: 29.9101505279541 seconds\n",
      "Creating BEV..\n",
      "Loading sparse influence matrix...\n",
      "Done\n",
      "Objective Start\n",
      "Objective done\n",
      "Constraints Start\n",
      "Constraints done\n",
      "Running Optimization..\n",
      "Optimal value: 1575.740944957925\n",
      "Elapsed time: 44.41001343727112 seconds\n",
      "Creating BEV..\n",
      "Loading sparse influence matrix...\n",
      "Done\n",
      "Objective Start\n",
      "Objective done\n",
      "Constraints Start\n",
      "Constraints done\n",
      "Running Optimization..\n",
      "Optimal value: 467.92110049420415\n",
      "Elapsed time: 69.50067138671875 seconds\n"
     ]
    }
   ],
   "source": [
    "portpy_env = my_env\n",
    "replay_buffer = rl_utils.ReplayBuffer(500)\n",
    "batch_size = 5\n",
    "minimal_size = 100\n",
    "state = portpy_env.reset()\n",
    "episode_return = 0\n",
    "done = False\n",
    "while not done:\n",
    "    # batch_normalization 不允许输入的批量数为1\n",
    "    action = random.randint\n",
    "    max_q_value = agent.max_q_value(\n",
    "        state) * 0.005 + max_q_value * 0.995  # 平滑处理\n",
    "    max_q_value_list.append(max_q_value)  # 保存每个状态的最大Q值\n",
    "    next_state, reward, done, _ = env.step([action])\n",
    "    replay_buffer.add(state[0], action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    episode_return += reward\n",
    "    if replay_buffer.size() > minimal_size:\n",
    "        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "        transition_dict = {\n",
    "            'states': b_s,\n",
    "            'actions': b_a,\n",
    "            'next_states': b_ns,\n",
    "            'rewards': b_r,\n",
    "            'dones': b_d\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c377beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "transition_dict = {\n",
    "    'states': b_s,\n",
    "    'actions': b_a,\n",
    "    'next_states': b_ns,\n",
    "    'rewards': b_r,\n",
    "    'dones': b_d\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "437c57b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 4.4357196e-08 2.2178598e-08\n",
      " 0.0000000e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1096"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = transition_dict['states'][0]\n",
    "print(a)\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d515da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, state):\n",
    "        # Splitting and restructuring of state to theta and dvh\n",
    "        # the feature extract of theta and dvh is done separately then merge two features into one\n",
    "        state_theta = state[:, :self.theta_dim]\n",
    "        theta_numpy = state_theta.numpy()\n",
    "        state_dvh = torch.reshape(state[:, self.theta_dim:],\n",
    "                                  (state.shape[0] , self.dvh_channels, self.dvh_dim)) \n",
    "        feature1 = self.feature1(state_theta)\n",
    "        feature2 = self.feature2(state_dvh)\n",
    "        feature = torch.cat((feature1, feature2), dim = 1)\n",
    "        x = F.relu(self.fc1(feature))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
